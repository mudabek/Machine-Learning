{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDfcLLVUld8d"
   },
   "source": [
    "# Player ratings based on the team results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZXqaAepld8n"
   },
   "source": [
    "In this notebook we will predict the player's invidual ratings based on the teams results. We will use dataset taken from popular in CIS team game called \"What? Where? When?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dDIjIN1Mld8p"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "import pdb\n",
    "import zipfile\n",
    "import operator\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fii9p5Kld8q"
   },
   "source": [
    "### 1. Processing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OlCjZoHlotY",
    "outputId": "3b37e329-b87a-4b26-aaff-0ff43d3881bd"
   },
   "outputs": [],
   "source": [
    "# load the needed files\n",
    "! wget https://www.dropbox.com/s/s4qj0fpsn378m2i/chgk.zip -nc\n",
    "\n",
    "with zipfile.ZipFile('chgk.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "tournaments = pickle.load(open('tournaments.pkl', 'rb'))\n",
    "results = pickle.load(open('results.pkl', 'rb'))\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gGB69E1xRWgu"
   },
   "outputs": [],
   "source": [
    "# get tournaments for training and testing model\n",
    "def get_tournaments_ids(year):\n",
    "    cur_tournaments = [v for k,v in tournaments.items() if v['dateStart'][:4] == year]\n",
    "    tournaments_with_results = [v for idx, v in enumerate(cur_tournaments) if v['id'] in results and results[v['id']] != []]\n",
    "    tournaments_with_results_mask = [v for idx, v in enumerate(tournaments_with_results) if 'mask' in results[v['id']][0]]\n",
    "    return [v['id'] for v in tournaments_with_results_mask]\n",
    "\n",
    "train_tournaments_id = set(get_tournaments_ids('2019'))\n",
    "test_tournaments_id = set(get_tournaments_ids('2020'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nVPQNJgBld8v"
   },
   "outputs": [],
   "source": [
    "# create a table for training and testing using the results file\n",
    "def make_table(tournaments_ids):\n",
    "    table = []\n",
    "    question_id = 0\n",
    "\n",
    "    for tourn_id, tourn_data in results.items():\n",
    "        if tourn_id in tournaments_ids:\n",
    "            # get number of questions in the tournament\n",
    "            numb_questions = 9999\n",
    "            for team in tourn_data:\n",
    "                if 'mask' in team and team['mask'] != None:\n",
    "                    if len(team['mask']) < numb_questions:\n",
    "                        numb_questions = len(team['mask'])\n",
    "            \n",
    "            for team in tourn_data:\n",
    "                if 'mask' in team and team['mask'] != None:\n",
    "                    total_answers = 0\n",
    "                    for answer in str(team['mask']):\n",
    "                        if answer == '1' or answer == '0':\n",
    "                            total_answers = total_answers + int(answer)\n",
    "\n",
    "                    team_id = team['team']['id']\n",
    "                    for member in team['teamMembers']:\n",
    "                        member_id = member['player']['id']\n",
    "                        for i in range(numb_questions):\n",
    "                            answer = team['mask'][i]\n",
    "                            if answer == '1' or answer == '0':\n",
    "                                table.append([tourn_id, team_id, member_id, question_id+i, int(answer), total_answers])\n",
    "            question_id = question_id + numb_questions           \n",
    "\n",
    "    return pd.DataFrame(table, columns=['tourn_id', 'team_id', 'p_id', 'q_id', 'answer', 'total_answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pj6Eed_Rld8w"
   },
   "outputs": [],
   "source": [
    "# get dataset for model training purposes\n",
    "df_train = make_table(train_tournaments_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgA7zrTold8x"
   },
   "source": [
    "### 2. Baseline Model for Players Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi27kZB4ld8y"
   },
   "source": [
    "We will train the model in the following way. We will use one hot encoding to represent our data. Columns will be players and questions. The target will be whether question was answered or not. As a result, coefficients of logistic regression model will correspond to players skills (higher coefficient of player column, better player's skill) and question complexity(higher coefficient of question column, easier the question). Using those coefficients we can approximately rate player's skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "LbM8au98ld8y"
   },
   "outputs": [],
   "source": [
    "# use one hot encoder from scikit\n",
    "one_hot_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train = one_hot_enc.fit_transform(df_train[['p_id', 'q_id']])\n",
    "y_train = np.array(df_train['answer'], dtype=np.int32)\n",
    "\n",
    "# train logistic regression \n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea1xbRXkld82"
   },
   "source": [
    "### 3. Model Accuracy Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkoDp23Jld82"
   },
   "source": [
    "We will predict rating of the teams based on the probability of at least one team member answering the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9MvCtMcJ0OMW"
   },
   "outputs": [],
   "source": [
    "# get dataset for model testing purposes\n",
    "df_test = make_table(test_tournaments_id)\n",
    "\n",
    "# keep only the players present in training set\n",
    "train_players = np.unique(df_train['p_id'])\n",
    "df_test = df_test[df_test.p_id.isin(train_players)]\n",
    "\n",
    "# remove question ids because we won't account for question complexity\n",
    "df_test['q_id_copy'] = df_test['q_id']\n",
    "df_test['q_id'] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E8-1gxxkWh09"
   },
   "outputs": [],
   "source": [
    "# get the model's probability predictions for answering the question\n",
    "X_test = one_hot_enc.transform(df_test[['p_id', 'q_id']])\n",
    "y_pred_prob = baseline_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IgGvK1q02l2p"
   },
   "outputs": [],
   "source": [
    "# calculate the predicted score based on probabilities of answering questions by the teammates\n",
    "df_test['answer_prob'] = y_pred_prob\n",
    "df_test['prob_score'] = df_test.groupby(['tourn_id', 'team_id', 'q_id_copy'])['answer_prob'].transform(lambda x: 1 - np.prod(1 - x))\n",
    "\n",
    "# create dataframe for comparing ratings\n",
    "df_ratings = df_test[['tourn_id', 'team_id', 'total_answers', 'prob_score']].drop_duplicates() \n",
    "\n",
    "# true rating is based on the number of questions team answered in the given tournament \n",
    "df_ratings = df_ratings.sort_values(by=['tourn_id', 'total_answers'], ascending=False) \n",
    "df_ratings['true_rating'] = df_ratings.groupby('tourn_id')['total_answers'].transform(lambda x: np.arange(1, len(x) + 1))\n",
    "\n",
    "# predicted rating is based on the probability scores calculated previously\n",
    "df_ratings = df_ratings.sort_values(by=['tourn_id', 'prob_score'], ascending=False) \n",
    "df_ratings['pred_rating'] = df_ratings.groupby('tourn_id')['prob_score'].transform(lambda x: np.arange(1, len(x) + 1))\n",
    "df_ratings['pred_rating'] = df_ratings['pred_rating'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfGXnFx765ta",
    "outputId": "0bce076c-b6d6-4b92-ec89-1eeac21a57e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmann correlation: 0.7884643695008277\n",
      "Kendall correlation: 0.6164033759517489\n"
     ]
    }
   ],
   "source": [
    "# calculate the correlation to check model's accuracy\n",
    "print(f\"Spearmann correlation: {df_ratings.groupby('tourn_id').apply(lambda x: spearmanr(x['true_rating'], x['pred_rating']).correlation).mean()}\")\n",
    "print(f\"Kendall correlation: {df_ratings.groupby('tourn_id').apply(lambda x: kendalltau(x['true_rating'], x['pred_rating']).correlation).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naNdskjj_gv0"
   },
   "source": [
    "### 4. EM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF-5LzefDN3g"
   },
   "source": [
    "We now will train EM model. We will introduce the latent variable z for each player-question pair. z = 0 for all teammates, when no one in the team answers the question. z = 1 for at least one teammate, when team correctly answers the question. <br>\n",
    "\n",
    "At the E-step we will calculate the expectation of z, by fixing players' skills and question complexities. At the M-step we will train logistic regression by setting z as target for each players. <br>\n",
    "\n",
    "The formula we can use at the E-step would be $P(z_{iq}=1) =  \\frac{\\sigma(x)}{1 - \\prod_{j\\in t}^{} (1 - \\sigma(x)))}$\n",
    ",where $z_{iq}$ is the result whether player q answers question i, $\\sigma$ is logistic regression model and $x$ is the input to the model. <br>\n",
    "\n",
    "Reference: Sergei Nikolenko, A probabilistic Rating System for Team Competitions with Individual Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRApzTirLLvU"
   },
   "outputs": [],
   "source": [
    "# get the dictionary of one-hots\n",
    "labels = one_hot_enc.categories_[0]\n",
    "one_hot_dict = {}\n",
    "for i in range(len(labels)): one_hot_dict[labels[i]] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lh545GM_ot4"
   },
   "outputs": [],
   "source": [
    "class EM() :\n",
    "    def __init__(self, learning_rate = 10, iterations = 10) :        \n",
    "        self.learning_rate = learning_rate        \n",
    "        self.iterations = iterations\n",
    "\n",
    "    def _quest_prob(self, q_id):\n",
    "        row_num = q_id#self.one_hot_dict[q_id]\n",
    "        return expit(self.X[row_num].dot(self.W) + self.b)\n",
    "\n",
    "    # E-step: calculate z-values fixing skill and complexity\n",
    "    def _E_step(self, data):\n",
    "        print('e started')\n",
    "        # invidiual question probabilities\n",
    "        self.team_info['q_prob'] = self.team_info.apply(lambda x: self._quest_prob(x['q_id']), axis=1)\n",
    "        print('prob')\n",
    "        # team question probabilities\n",
    "        self.team_info['t_prob'] = self.team_info.groupby(['team_id', 'q_id'])['q_prob'].transform(lambda x: np.prod(1 - x))\n",
    "        self.team_info['t_prob'] = self.team_info.groupby('t_prob').apply(lambda x: 1 - x)\n",
    "        print('group')\n",
    "        # update latent variable Z\n",
    "        self.Z = (self.team_info['q_prob'] / self.team_info['t_prob']).to_numpy()\n",
    "        self.Z[self.Y == 0] = 0\n",
    "        print('e-step done')\n",
    "        \n",
    "\n",
    "    # M-step: train log regression with z-values as targets\n",
    "    def _M_step(self, iter=1):\n",
    "        for i in range(iter):\n",
    "            self._update_weights()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _update_weights(self) :           \n",
    "        A = expit(self.X.dot(self.W) + self.b) \n",
    "          \n",
    "        # calculate gradients        \n",
    "        tmp = (A - self.Z.T)        \n",
    "        tmp = np.reshape(tmp, self.m)        \n",
    "        dW = np.dot(self.X.T, tmp) / self.m         \n",
    "        db = np.sum(tmp) / self.m \n",
    "          \n",
    "        # update weights    \n",
    "        self.W = self.W - self.learning_rate * dW    \n",
    "        self.b = self.b - self.learning_rate * db\n",
    "          \n",
    "        return self\n",
    "\n",
    "          \n",
    "    # Function for model training    \n",
    "    def fit(self, X, Y, data, one_hot_dict) :        \n",
    "        # no_of_training_examples, no_of_features        \n",
    "        self.m, self.n = X.shape        \n",
    "        # weight initialization        \n",
    "        self.W = np.zeros(self.n)        \n",
    "        self.b = 0        \n",
    "        self.X = X        \n",
    "        self.Y = Y\n",
    "        self.Z = Y\n",
    "        self.one_hot_dict = one_hot_dict\n",
    "\n",
    "        # helper frame initialization\n",
    "        self.team_info = pd.DataFrame({'team_id': data['team_id'], 'q_id': data['q_id']})\n",
    "        self.team_info['q_prob'] = self.Y\n",
    "        self.team_info['t_prob'] = self.Y\n",
    "        data['z'] = self.Z\n",
    "          \n",
    "        # gradient descent learning\n",
    "        for i in range(self.iterations) :            \n",
    "            self.Z = self._E_step(data) \n",
    "            self._M_step()            \n",
    "        \n",
    "        return self\n",
    "      \n",
    "    def predict(self) :    \n",
    "        temp = expit(self.X.dot(self.W) + self.b)  \n",
    "        Y = np.where(temp > 0.5, 1, 0 )           \n",
    "        return Y\n",
    "    \n",
    "    def predict_prob(self):\n",
    "        return expit(self.X.dot(self.W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAUzeYLyJC1F"
   },
   "outputs": [],
   "source": [
    "em_model = EM(iterations=1)\n",
    "em_model.fit(X_train, y_train, df_train, one_hot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, don't have enough RAM to check whether the implementation is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tournament ratings based on question complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure the rating of the tournament based on the average complexity of the questions. In the baseline model we have coefficicients of each question. Using those coefficients we will approximate average question complexity of the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tournament names for displaying results\n",
    "df_tournaments = pd.DataFrame(pd.DataFrame(pd.read_pickle('tournaments.pkl')).T)\n",
    "df_tournaments['dateStart'] =  pd.to_datetime(df_tournaments['dateStart'],utc=True).dt.year\n",
    "df_tournaments = df_tournaments[df_tournaments['dateStart'] == 2019]\n",
    "df_tournaments = df_tournaments.drop(columns=['dateEnd', 'dateStart', 'type','season','orgcommittee','synchData','questionQty'])\n",
    "tourn_dict = dict(zip(df_tournaments.id, df_tournaments.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find question average complexity for tournaments and sort results\n",
    "questions = np.unique(df_train['q_id'])\n",
    "q_ratings = dict(zip(questions, baseline_model.coef_[0][-len(questions):]))\n",
    "df_train['q_diffic'] = df_train['q_id'].map(q_ratings)\n",
    "t_ratings = df_train[['tourn_id', 'q_id', 'q_diffic']].drop_duplicates()\n",
    "t_ratings = t_ratings.groupby('tourn_id')['q_diffic'].mean().sort_values()\n",
    "ratings_list = t_ratings.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toughest 20 tournaments:\n",
      "\n",
      "Чемпионат Санкт-Петербурга. Первая лига\n",
      "Угрюмый Ёрш\n",
      "Синхрон высшей лиги Москвы\n",
      "Первенство правого полушария\n",
      "Воображаемый музей\n",
      "Записки охотника\n",
      "Знание – Сила VI\n",
      "Ускользающая сова\n",
      "Кубок городов\n",
      "Чемпионат Мира. Этап 2. Группа В\n",
      "Чемпионат Минска. Лига А. Тур четвёртый\n",
      "VERSUS: Коробейников vs. Матвеев\n",
      "All Cats Are Beautiful\n",
      "Антибинго\n",
      "Чемпионат Мира. Этап 2 Группа С\n",
      "Чемпионат России\n",
      "Львов зимой. Адвокат\n",
      "Чемпионат Мира. Этап 3. Группа В\n",
      "Кубок Москвы\n",
      "Чемпионат Мира. Этап 1. Группа С\n",
      "\n",
      "\n",
      "\n",
      "Easiest 20 tournaments:\n",
      "\n",
      "(а)Синхрон-lite. Лига старта. Эпизод V\n",
      "Синхрон Лиги Разума\n",
      "(а)Синхрон-lite. Лига старта. Эпизод III\n",
      "(а)Синхрон-lite. Лига старта. Эпизод IX\n",
      "(а)Синхрон-lite. Лига старта. Эпизод VI\n",
      "(а)Синхрон-lite. Лига старта. Эпизод VII\n",
      "Второй тематический турнир имени Джоуи Триббиани\n",
      "(а)Синхрон-lite. Лига старта. Эпизод X\n",
      "(а)Синхрон-lite. Лига старта. Эпизод IV\n",
      "Синхрон-lite. Выпуск XXX\n",
      "Joystick Cup\n",
      "Синхрон-lite. Выпуск XXIX\n",
      "Лига Сибири. VI тур.\n",
      "Лига Сибири. IV тур.\n",
      "Лига вузов. IV тур\n",
      "Ничто, нигде, никогда\n",
      "Гарри Поттер и 3 по 12\n",
      "Маленькае люстэрка\n",
      "Школьный Синхрон-lite. Выпуск 2.5\n"
     ]
    }
   ],
   "source": [
    "top_n = 20\n",
    "# print top top_n tournaments\n",
    "print(f\"Toughest {top_n} tournaments:\\n\")\n",
    "for i in range(top_n): print(tourn_dict[ratings_list[i]]) \n",
    "\n",
    "print(f\"\\n\\n\\nEasiest {top_n} tournaments:\\n\")\n",
    "# print lowest top_n tournaments\n",
    "for i in range(1, top_n): print(tourn_dict[ratings_list[-i]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Players with few games but high ratings (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this kind of outliers affectings the model results we can set a certain threshold after which the players answer results will start counting towards the rating system. For example, we can say that after 100 answered questions we will start accoutning for players contribution towards player rating system. <br>\n",
    "\n",
    "Another possibility is adding number of answered questions to the logistic regression model. We will add one more column that will keep track of how many questions player answered and depending on that model will have certain coefficients that we can use to adjust the rating score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Accounting for players skill improvements over time (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to account less for the past results and more for the recent ones. This can be done by dropping out the results from the past. We can say that games played long time ago should have a a high dropout probability, while the games played recently will have low probability. In this way our model will account more for the recently played results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Player rating",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
