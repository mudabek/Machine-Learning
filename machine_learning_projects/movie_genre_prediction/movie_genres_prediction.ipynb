{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have textual dialogue data taken from IMDB. Dialogue texts map to respective movie genres. Goal of the model is to predict movie genre based on the given dialogue text from the movie. \n",
    "\n",
    "In the separate notebook dialogues were processed. After different models were built using different libraries. You can see those models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.pyplot.style.use('ggplot')\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics, model_selection, pipeline, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import processed train and test data ###\n",
    "\n",
    "train_data_stem = pd.read_csv('clean_train_stem.csv')\n",
    "test_data_stem = pd.read_csv('clean_test_stem.csv')\n",
    "train_data_lemma = pd.read_csv('clean_train_lemma.csv')\n",
    "test_data_lemma = pd.read_csv('clean_test_lemma.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_choose(text):\n",
    "    l = text.split(\",\")\n",
    "    text = random.choice(l).strip()\n",
    "    return text\n",
    "\n",
    "train_data_stem['genres'] = train_data_stem['genres'].apply(lambda x: split_and_choose(x))\n",
    "train_data_lemma['genres'] = train_data_lemma['genres'].apply(lambda x: split_and_choose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stem, holdout_data_stem = model_selection.train_test_split(train_data_stem, test_size = 0.2, random_state = 1)\n",
    "data_lemma, holdout_data_lemma = model_selection.train_test_split(train_data_lemma, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 3, stop_words={'english'})\n",
    "vectorized_train_data = vectorizer.fit_transform(data_stem['dialogue'])\n",
    "vectorized_test_data = vectorizer.transform(holdout_data_stem['dialogue'])\n",
    "vectorized_final_data = vectorizer.transform(test_data_stem['dialogue'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(max_iter=1000)\n",
    "model.fit(vectorized_train_data, data_stem['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(vectorized_train_data)\n",
    "test_preds = model.predict(vectorized_test_data)\n",
    "final_preds = model.predict(vectorized_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_true_train = data_stem['genres'].to_numpy()\n",
    "np_true_test = holdout_data_stem['genres'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train accuracy: {metrics.accuracy_score(np_true_train, train_preds):.3f}')\n",
    "print(f'test accuracy: {metrics.accuracy_score(np_true_test, test_preds):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.read_csv('sample_submission_most_popular.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['genres'] = final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.to_csv('stem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "train_doc, test_doc = model_selection.train_test_split(train_data_stem, random_state=0, test_size=0.3)\n",
    "X_train = train_doc['dialogue']\n",
    "X_test = test_doc['dialogue']\n",
    "y_train = train_doc['genres']\n",
    "y_test = test_doc['genres']\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 100, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 100, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = logreg.predict(train_vectors_dbow)\n",
    "y_holdout_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train accuracy: {metrics.accuracy_score(y_train , y_train_pred):.3f}')\n",
    "print(f'test accuracy: {metrics.accuracy_score(y_test , y_holdout_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stem, holdout_data_stem = model_selection.train_test_split(train_data_stem, test_size = 0.2, random_state = 1)\n",
    "#data_lemma, holdout_data_lemma = model_selection.train_test_split(train_data_lemma, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(strip_accents = 'unicode', stop_words = {'english'},\n",
    "                             min_df = 3, max_df = 6000,\n",
    "                             analyzer='word', token_pattern=r'\\w{2,}')#, max_features=9000)\n",
    "tfidf_vect.fit(data_stem['dialogue'].values.astype('U'))\n",
    "tfidf_train = tfidf_vect.transform(data_stem['dialogue'].values.astype('U'))\n",
    "tfidf_test = tfidf_vect.transform(holdout_data_stem['dialogue'].values.astype('U'))\n",
    "tfidf_final = tfidf_vect.transform(test_data_stem['dialogue'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_model = linear_model.LogisticRegression(max_iter=1000)\n",
    "tfidf_model.fit(tfidf_train, data_stem['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = tfidf_model.predict(tfidf_train)\n",
    "test_preds = tfidf_model.predict(tfidf_test)\n",
    "final_preds = tfidf_model.predict(tfidf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_true_train = data_stem['genres'].to_numpy()\n",
    "np_true_test = holdout_data_stem['genres'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train accuracy: {metrics.accuracy_score(np_true_train, train_preds):.3f}')\n",
    "print(f'test accuracy: {metrics.accuracy_score(np_true_test, test_preds):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.read_csv('sample_submission_most_popular.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['genres'] = final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.to_csv('tf-idf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stem['genres'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
