{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is scraped of airbnb website. It contains various features listed in the end of this markdown. Also, we have calendar containing availability of houses at various periods of time. And lastly we have text reviews by customers regarding the house they stayed in. The goal of the model is to predict housing prices as close as plossible minimizing the MAPE (mean absolute percentage error). Due to diversity of data presented (textual, categorical and numerical) different preprocessing techniques were used.\n",
    "\n",
    "\n",
    "\n",
    "*Features contained in train.csv:*<br>\n",
    "id,name,summary,space,description,experiences_offered,neighborhood_overview,notes,transit,access,\n",
    "interaction,house_rules,host_id,host_since,host_about,host_response_time,host_response_rate,\n",
    "host_is_superhost,host_has_profile_pic,host_identity_verified,neighbourhood_cleansed,zipcode,\n",
    "latitude,longitude,is_location_exact,property_type,room_type,accommodates,bathrooms,bedrooms,\n",
    "beds,bed_type,amenities,square_feet,security_deposit,cleaning_fee,guests_included,extra_people,\n",
    "minimum_nights,cancellation_policy,require_guest_profile_picture,require_guest_phone_verification,price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8N8WDZ3lLfS",
    "outputId": "d756fa5c-f55c-47bc-d840-27afb5aeb79a"
   },
   "outputs": [],
   "source": [
    "# install geopandas if needed\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZf6WqvNt1uF"
   },
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import datetime \n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import math\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics, model_selection, pipeline, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import tree, ensemble, model_selection, linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, LGBMModel\n",
    "\n",
    "# graphing defaults setup\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EODkYQwbt1uF"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbIMMunkxho4",
    "outputId": "f6fb09fd-cfcd-4305-c7c5-cb805e4a1735"
   },
   "outputs": [],
   "source": [
    "# for google collab usage\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path = '/content/drive/MyDrive/ML/Houses/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QiIV0tJ7W52"
   },
   "outputs": [],
   "source": [
    "### Processing reviews and importing already sentimented reviews ###\n",
    "\n",
    "sentiments_csv = pd.read_csv(path + 'sentimented_reviews.csv')\n",
    "data_reviews_csv = pd.read_csv(path + 'reviews.csv', parse_dates=['date'])\n",
    "last_date = data_reviews_csv['date'].max().date()\n",
    "\n",
    "first_review = pd.DataFrame(data_reviews_csv.groupby(by = ['listing_id'])['date'].min())\n",
    "last_review = pd.DataFrame(data_reviews_csv.groupby(by =['listing_id'])['date'].max())\n",
    "\n",
    "days_since_first_review = first_review.applymap(lambda x:(last_date - x.date()).days)\n",
    "days_since_last_review = last_review.applymap(lambda x:(last_date - x.date()).days)\n",
    "\n",
    "sentiments_csv.loc[:, 'LastReview'] = days_since_last_review.reset_index(drop = False)['date']\n",
    "sentiments_csv.loc[:, 'FirstReview'] = days_since_first_review.reset_index(drop = False)['date']\n",
    "\n",
    "count = pd.DataFrame(data_reviews_csv['listing_id'].value_counts())\n",
    "sentiments_csv['frequency'] = sentiments_csv.listing_id.map(dict(zip(count.index, count.listing_id)))\n",
    "sentiments_csv['days_active'] = sentiments_csv.FirstReview - sentiments_csv.LastReview\n",
    "sentiments_csv.drop(columns=['comments', 'LastReview', 'FirstReview'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBEl7XqSt1uF"
   },
   "outputs": [],
   "source": [
    "### Function for cleaning train and test data ###\n",
    "\n",
    "CENTER_LAT = radians(51.49757)\n",
    "CENTER_LONG = radians(-0.13585)\n",
    "R = 6373.0\n",
    "infrequent_amenities = []\n",
    "\n",
    "def process_amenities(df):\n",
    "  df.loc[df['amenities'].str.contains('24-hour check-in'), 'check_in_24h'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Air conditioning|Central air conditioning'), 'air_conditioning'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Amazon Echo|Apple TV|Game console|Netflix|Projector and screen|Smart TV'), 'high_end_electronics'] = 1\n",
    "  df.loc[df['amenities'].str.contains('BBQ grill|Fire pit|Propane barbeque'), 'bbq'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Balcony|Patio'), 'balcony'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Beach view|Beachfront|Lake access|Mountain view|Ski-in/Ski-out|Waterfront'), 'nature_and_views'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Bed linens'), 'bed_linen'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Breakfast'), 'breakfast'] = 1\n",
    "  df.loc[df['amenities'].str.contains('TV'), 'tv'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Coffee maker|Espresso machine'), 'coffee_machine'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Cooking basics'), 'cooking_basics'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Dishwasher|Dryer|Washer'), 'white_goods'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Elevator'), 'elevator'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Exercise equipment|Gym|gym'), 'gym'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Family/kid friendly|Children|children'), 'child_friendly'] = 1\n",
    "  df.loc[df['amenities'].str.contains('parking'), 'parking'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Garden|Outdoor|Sun loungers|Terrace'), 'outdoor_space'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Host greets you'), 'host_greeting'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Hot tub|Jetted tub|hot tub|Sauna|Pool|pool'), 'hot_tub_sauna_or_pool'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Internet|Pocket wifi|Wifi'), 'internet'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Long term stays allowed'), 'long_term_stays'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Pets|pet|Cat(s)|Dog(s)'), 'pets_allowed'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Private entrance'), 'private_entrance'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Safe|Security system'), 'secure'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Self check-in'), 'self_check_in'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Smoking allowed'), 'smoking_allowed'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Step-free access|Wheelchair|Accessible'), 'accessible'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Suitable for events'), 'event_suitable'] = 1\n",
    "  # Replacing nulls with zeros for new columns\n",
    "  cols_to_replace_nulls = df.iloc[:,41:].columns\n",
    "  df[cols_to_replace_nulls] = df[cols_to_replace_nulls].fillna(0)\n",
    "\n",
    "  # Produces a list of amenity features where one category (true or false) contains fewer than 10% of listings\n",
    "  \n",
    "  for col in df.iloc[:,41:].columns:\n",
    "      if df[col].value_counts()[1] < len(df)/10:\n",
    "          infrequent_amenities.append(col)\n",
    "\n",
    "  # Dropping infrequent amenity features\n",
    "  df.drop(infrequent_amenities, axis=1, inplace=True)\n",
    "\n",
    "  # Dropping the original amenity feature\n",
    "  df.drop('amenities', axis=1, inplace=True)\n",
    "  return df\n",
    "\n",
    "def process_test_amenities(df):\n",
    "  df.loc[df['amenities'].str.contains('24-hour check-in'), 'check_in_24h'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Air conditioning|Central air conditioning'), 'air_conditioning'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Amazon Echo|Apple TV|Game console|Netflix|Projector and screen|Smart TV'), 'high_end_electronics'] = 1\n",
    "  df.loc[df['amenities'].str.contains('BBQ grill|Fire pit|Propane barbeque'), 'bbq'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Balcony|Patio'), 'balcony'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Beach view|Beachfront|Lake access|Mountain view|Ski-in/Ski-out|Waterfront'), 'nature_and_views'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Bed linens'), 'bed_linen'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Breakfast'), 'breakfast'] = 1\n",
    "  df.loc[df['amenities'].str.contains('TV'), 'tv'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Coffee maker|Espresso machine'), 'coffee_machine'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Cooking basics'), 'cooking_basics'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Dishwasher|Dryer|Washer'), 'white_goods'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Elevator'), 'elevator'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Exercise equipment|Gym|gym'), 'gym'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Family/kid friendly|Children|children'), 'child_friendly'] = 1\n",
    "  df.loc[df['amenities'].str.contains('parking'), 'parking'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Garden|Outdoor|Sun loungers|Terrace'), 'outdoor_space'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Host greets you'), 'host_greeting'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Hot tub|Jetted tub|hot tub|Sauna|Pool|pool'), 'hot_tub_sauna_or_pool'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Internet|Pocket wifi|Wifi'), 'internet'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Long term stays allowed'), 'long_term_stays'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Pets|pet|Cat(s)|Dog(s)'), 'pets_allowed'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Private entrance'), 'private_entrance'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Safe|Security system'), 'secure'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Self check-in'), 'self_check_in'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Smoking allowed'), 'smoking_allowed'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Step-free access|Wheelchair|Accessible'), 'accessible'] = 1\n",
    "  df.loc[df['amenities'].str.contains('Suitable for events'), 'event_suitable'] = 1\n",
    "  # Replacing nulls with zeros for new columns\n",
    "  cols_to_replace_nulls = df.iloc[:,41:].columns\n",
    "  df[cols_to_replace_nulls] = df[cols_to_replace_nulls].fillna(0)\n",
    "\n",
    "  # Dropping infrequent amenity features\n",
    "  df.drop(infrequent_amenities, axis=1, inplace=True)\n",
    "\n",
    "  # Dropping the original amenity feature\n",
    "  df.drop('amenities', axis=1, inplace=True)\n",
    "  return df\n",
    "\n",
    "\n",
    "#GeoData on London's hoods\n",
    "map_df = gpd.read_file(path + 'neighbourhoods.geojson')\n",
    "del map_df['neighbourhood_group']\n",
    "map_df.head(5)\n",
    "subset = map_df[['neighbourhood', 'geometry']]\n",
    "areas = [tuple(x) for x in subset.to_numpy()]\n",
    "\n",
    "def get_hood(df):\n",
    "    gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.longitude, df.latitude))\n",
    "    points = gdf.geometry\n",
    "    hoods = []\n",
    "\n",
    "    for row in points:\n",
    "      found = False\n",
    "      for i in range(len(areas)):\n",
    "        if (areas[i][1].contains(row)):\n",
    "          hoods.append(areas[i][0])\n",
    "          found = True\n",
    "          break\n",
    "      if (not found):\n",
    "          hoods.append('NAN')\n",
    "      \n",
    "    return pd.DataFrame(hoods)\n",
    "    \n",
    "def correct_zip(zipcode):\n",
    "    result = re.search(\"([A-Z]+[0-9]+)\", zipcode)\n",
    "    if (result == None):\n",
    "      return 'NAN'\n",
    "    else:\n",
    "      return result.group()\n",
    "\n",
    "def process_zipcode(df):\n",
    "    f = lambda row : correct_zip(str(row['zipcode']).upper().split(' ')[0])\n",
    "    df = df.apply(f, axis=1)\n",
    "    return df\n",
    "\n",
    "def change_nan(data):\n",
    "    if math.isnan(data):\n",
    "        data = -1\n",
    "    return data\n",
    "\n",
    "def change_2(data):\n",
    "    if data > 1.0:\n",
    "        data = 1.0\n",
    "    return data\n",
    "\n",
    "def process_2(df):\n",
    "    df = df.apply(lambda x: change_2(x))\n",
    "    return df\n",
    "\n",
    "def normalize(df):\n",
    "    result = pd.DataFrame(df.copy())\n",
    "    print(type(result))\n",
    "    for feature_name in result:\n",
    "        max_value = result[feature_name].max()\n",
    "        min_value = result[feature_name].min()\n",
    "        result[feature_name] = (result[feature_name] - min_value) \\\n",
    "                / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    #exclude = set(string.punctuation)\n",
    "    #no_punct = \" \".join([c for c in text if c not in exclude])\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    return text.translate(translator)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text\n",
    "\n",
    "def prep_for_vect(df):\n",
    "    df = df.apply(lambda x: remove_punctuation(x))\n",
    "    df = df.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    df = df.apply(lambda x: ' '. join(x))\n",
    "    #df = df.apply(lambda x: word_stemmer(x))\n",
    "    return df\n",
    "\n",
    "def word_to_vec(df):\n",
    "    v = TfidfVectorizer()\n",
    "    x = v.fit_transform(prep_for_vect(df))\n",
    "    df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
    "    return df1.loc[:, v.get_feature_names()[0]:]. \\\n",
    "        apply(lambda x : x.tolist(), axis=1)\n",
    "\n",
    "def get_distance_from_center(lat, long):\n",
    "    lat = radians(lat)\n",
    "    long = radians(long)\n",
    "    dlon = CENTER_LONG - long\n",
    "    dlat = CENTER_LAT - lat\n",
    "    a = sin(dlat / 2)**2 + cos(lat) * cos(CENTER_LAT) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "    \n",
    "def process_geo(lat, long):\n",
    "    #norm_lat = normalize(np.radians(lat))\n",
    "    #norm_long = normalize(np.radians(long))\n",
    "    #long = pd.DataFrame(long, columns=['longitude'])\n",
    "    #lat = pd.DataFrame(lat, columns=['latitude'])\n",
    "    result = pd.concat([long, lat], axis = 1)\n",
    "    #return result.loc[:, :].apply(lambda x : x.tolist(), axis=1)\n",
    "    result = pd.DataFrame(result.apply( \\\n",
    "        lambda row : get_distance_from_center( \\\n",
    "        row['latitude'], row['longitude']), axis=1))\n",
    "    return result\n",
    "\n",
    "def normalize_frame(dataset):\n",
    "    dataNorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))\n",
    "    return dataNorm\n",
    "\n",
    "def encode(dataset, title):\n",
    "    enum_data = pd.factorize(dataset)[0]\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset[title] = enum_data\n",
    "    return pd.get_dummies(dataset[title])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def vectorize_train_text(df):\n",
    "    df['room_type'] = df['room_type'].astype(str)\n",
    "    df['hood'] = df['hood'].astype(str)\n",
    "    df['text'] = df[['room_type', 'hood']].agg(' '.join, axis=1)\n",
    "    del df['room_type']\n",
    "    vectorizer.fit(df['text'])\n",
    "    X = vectorizer.transform(df['text'])\n",
    "    matrix = X.toarray()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, pd.DataFrame(matrix).reset_index(drop=True)], axis = 1)\n",
    "    del df['text']\n",
    "\n",
    "    return df\n",
    "\n",
    "def vectorize_test_text(df): \n",
    "    df['room_type'] = df['room_type'].astype(str)\n",
    "    df['hood'] = df['hood'].astype(str)\n",
    "    df['text'] = df[['room_type', 'hood']].agg(' '.join, axis=1)\n",
    "    del df['room_type']\n",
    "\n",
    "    X = vectorizer.transform(df['text'])\n",
    "    matrix = X.toarray()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, pd.DataFrame(matrix).reset_index(drop=True)], axis = 1)\n",
    "    del df['text']\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_column(df, feature_name):\n",
    "    result = df.copy()\n",
    "    max_value = df[feature_name].max()\n",
    "    min_value = df[feature_name].min()\n",
    "    result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def get_middle(df,percent):\n",
    "\n",
    "    start = int(len(df)*percent)\n",
    "    end = len(df) - start\n",
    "\n",
    "    return df.iloc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo2VK86St1uF"
   },
   "outputs": [],
   "source": [
    "### Pipeline for train data processing ###\n",
    "\n",
    "class TrainDataCleaner(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params): \n",
    "        result = pd.DataFrame(X['id'])\n",
    "        #clean up simple numeric data\n",
    "        guests_included = pd.to_numeric(X.guests_included)\n",
    "        accommodates = pd.to_numeric(X.accommodates)\n",
    "        #zipcode = pd.DataFrame(process_zipcode(X))\n",
    "        bathrooms = pd.to_numeric(X.bathrooms.fillna(X.bathrooms.median()))\n",
    "        bedrooms = pd.to_numeric(X.bedrooms.fillna(X.bedrooms.mean()))\n",
    "        beds = pd.to_numeric(X.beds.fillna(X.beds.mean()))\n",
    "        \n",
    "        #geodata processing\n",
    "        lat = X['latitude']\n",
    "        long = X['longitude']\n",
    "        distance_from_center = process_geo(lat, long)\n",
    "        \n",
    "        #collect numeric data and normalize\n",
    "        result['guests_included'] = guests_included\n",
    "        result['accomodates'] = accommodates\n",
    "        result['bathroom'] = bathrooms\n",
    "        result['bedrooms'] = bedrooms\n",
    "        result['beds'] = beds\n",
    "        result['distance_from_center'] = distance_from_center\n",
    "        result = normalize_frame(result)\n",
    "\n",
    "        #remove values without zipcode, and no prices\n",
    "        result['amenities'] = X.amenities\n",
    "        result = process_amenities(result)\n",
    "        result['room_type'] = X.room_type\n",
    "        result['price'] = X.price\n",
    "        result['hood'] = get_hood(X)\n",
    "        result = vectorize_train_text(result)\n",
    "        result['listing_id'] = X['id']\n",
    "        result = result.join(sentiments_csv.set_index('listing_id'), on='listing_id')\n",
    "        \n",
    "        result = result[result.price != 0]\n",
    "        result = result[result.frequency > 3]\n",
    "        result = result[result.days_active != 0]\n",
    "        result = normalize_column(result, 'frequency')\n",
    "        result = normalize_column(result, 'days_active')\n",
    "        result = result[result.hood != 'NAN']\n",
    "        result.drop(columns=['sentiment', 'hood', 'id', 'listing_id'], inplace=True)\n",
    "\n",
    "        #vectorize textual data      \n",
    "        \n",
    "        result.fillna(value=0, inplace=True)\n",
    "        \n",
    "\n",
    "        return result\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "train_data_cleaner = Pipeline([\n",
    "    ('clean data', TrainDataCleaner())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeag0ObNwKMd"
   },
   "outputs": [],
   "source": [
    "### Pipeline for cleaning test data ###\n",
    "\n",
    "class TestDataCleaner(TransformerMixin):\n",
    "\n",
    "  def transform(self, X, **transform_params): \n",
    "        result = pd.DataFrame(X['id'])\n",
    "        #clean up simple numeric data\n",
    "        guests_included = pd.to_numeric(X.guests_included)\n",
    "        accommodates = pd.to_numeric(X.accommodates)\n",
    "        #zipcode = pd.DataFrame(process_zipcode(X))\n",
    "        bathrooms = pd.to_numeric(X.bathrooms.fillna(X.bathrooms.median()))\n",
    "        bedrooms = pd.to_numeric(X.bedrooms.fillna(X.bedrooms.mean()))\n",
    "        beds = pd.to_numeric(X.beds.fillna(X.beds.mean()))\n",
    "        \n",
    "        #geodata processing\n",
    "        lat = X['latitude']\n",
    "        long = X['longitude']\n",
    "        distance_from_center = process_geo(lat, long)\n",
    "        \n",
    "        #collect numeric data and normalize\n",
    "        result['guests_included'] = guests_included\n",
    "        result['accomodates'] = accommodates\n",
    "        result['bathroom'] = bathrooms\n",
    "        result['bedrooms'] = bedrooms\n",
    "        result['beds'] = beds\n",
    "        result['distance_from_center'] = distance_from_center\n",
    "        result = normalize_frame(result)\n",
    "\n",
    "        #remove values without zipcode, and no prices\n",
    "        result['amenities'] = X.amenities\n",
    "        result = process_test_amenities(result)\n",
    "        result['room_type'] = X.room_type\n",
    "        result['hood'] = get_hood(X)\n",
    "        result = vectorize_test_text(result)\n",
    "        result['listing_id'] = X['id']\n",
    "        result = result.join(sentiments_csv.set_index('listing_id'), on='listing_id')\n",
    "\n",
    "        result.loc[result['frequency'] < 3, 'frequency'] = result['frequency'].median()\n",
    "        result.loc[result['days_active'] == 0, 'days_active'] = result['days_active'].median()\n",
    "\n",
    "        result = normalize_column(result, 'frequency')\n",
    "        result = normalize_column(result, 'days_active')\n",
    "        result.drop(columns=['sentiment', 'hood', 'id', 'listing_id'], inplace=True)\n",
    "\n",
    "        #vectorize textual data      \n",
    "        \n",
    "        result.fillna(value=0, inplace=True)\n",
    "\n",
    "\n",
    "        return result\n",
    "        \n",
    "  def fit(self, X, y=None, **fit_params):\n",
    "      return self\n",
    "    \n",
    "test_data_cleaner = Pipeline([\n",
    "    ('clean data', TestDataCleaner())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9hoNuzRt1uG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKryDNEpXTMy"
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    mask = np.where(y_true==0, True, False) \n",
    "    return np.nanmean(np.abs((y_true[~mask] - y_pred[~mask]) / y_true[~mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZvzTjK1t1uG",
    "outputId": "1ae8449e-4e91-442a-b119-3acf19182ba0"
   },
   "outputs": [],
   "source": [
    "data_train_csv = pd.read_csv(path + 'train.csv')\n",
    "part = int(len(data_train_csv) * 0.8)\n",
    "train_data_raw = data_train_csv.iloc[:part]\n",
    "test_data_raw = data_train_csv.iloc[part:]\n",
    "\n",
    "train_data = train_data_cleaner.transform(train_data_raw)\n",
    "test_data = test_data_cleaner.transform(test_data_raw)\n",
    "\n",
    "train_target = train_data['price']\n",
    "test_target = test_data_raw['price']\n",
    "\n",
    "train_data.drop(columns=['price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZ2F5q5nt1uG",
    "outputId": "1c540259-a5e9-4591-8893-0169820523ce"
   },
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42, sampling_strategy='majority')\n",
    "X_res, y_res = rus.fit_resample(train_data, train_target)\n",
    "\n",
    "rf_model = ensemble.RandomForestClassifier(n_estimators = 90, min_samples_split = 15, random_state = 1)\n",
    "rf_model.fit(X_res, y_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm-A5DgJt1uG"
   },
   "outputs": [],
   "source": [
    "train_pred_np = rf_model.predict(train_data)\n",
    "test_pred_np = rf_model.predict(test_data)\n",
    "test_target_np = test_target.to_numpy()\n",
    "train_target_np = train_target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wqyxp_Jft1uH",
    "outputId": "895a0ac0-3b71-483c-ebc3-cc96aeb36503"
   },
   "outputs": [],
   "source": [
    "print(f'train: {mean_absolute_percentage_error(train_target_np, train_pred_np)}')\n",
    "print(f'test: {mean_absolute_percentage_error(test_target_np, test_pred_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ1M8ejfAkw4"
   },
   "source": [
    "# Final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N3JaTBKQ_Wq"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSVnERMcRZQh",
    "outputId": "c53fdec2-d7d0-43c0-ecfb-81fde4bf2c13"
   },
   "outputs": [],
   "source": [
    "data_test_csv = pd.read_csv(path + 'test.csv')\n",
    "final_test_data = test_data_cleaner.transform(data_test_csv)\n",
    "final_pred = rf_model.predict(final_test_data)\n",
    "submission['price'] = final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOprg1bBSfwz"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(path + 'random_forest.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_61cTTVKoUaU"
   ],
   "name": "Copy of Copy of House Price Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
